{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a good receipie?\n",
    "first we need to import the revelevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to start scraping we will first get all the receipie names and links and save them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_page_thespruceeats():#this returns a list of all the links to receipies on the page:\n",
    "    # URL to scrape\n",
    "    url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\n",
    "\n",
    "    # Configure the Selenium webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    results_div = soup.find(\"div\", attrs={\"class\": \"results-list__container\"})\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "\n",
    "    # Scrape the first page\n",
    "    for li in results_div.find_all(\"li\", class_=\"results__item\"):\n",
    "        if li.find(\"a\") is not None:\n",
    "            link = li.find(\"a\").get(\"href\")\n",
    "        else:\n",
    "            link = ''\n",
    "\n",
    "        if li.find(\"h4\", class_=\"card__title\") is not None:\n",
    "            name = li.find(\"h4\", class_=\"card__title\").text.strip()\n",
    "        else:\n",
    "            name = ''\n",
    "        \n",
    "        recipe_names.append(name)\n",
    "        recipe_links.append(link)\n",
    "\n",
    "    # Scrape subsequent pages if the \"Next\" button exists\n",
    "    while True:\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".pagination__item-link--next\")))\n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            results_div = soup.find(\"div\", attrs={\"class\": \"results-list__container\"})\n",
    "\n",
    "            # Scrape recipe names and links\n",
    "            for li in results_div.find_all(\"li\", class_=\"results__item\"):\n",
    "                if li.find(\"a\") is not None:\n",
    "                    link = li.find(\"a\").get(\"href\")\n",
    "                else:\n",
    "                    link = ''\n",
    "\n",
    "                if li.find(\"h4\", class_=\"card__title\") is not None:\n",
    "                    name = li.find(\"h4\", class_=\"card__title\").text.strip()\n",
    "                else:\n",
    "                    name = ''\n",
    "                print(f\"{name} added\")\n",
    "                recipe_names.append(name)\n",
    "                recipe_links.append(link)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame with the recipe names and links\n",
    "    df = pd.DataFrame({\"Recipe_name\": recipe_names, \"Recipe_link\": recipe_links})\n",
    "\n",
    "    # Write DataFrame to a CSV file\n",
    "    df.to_csv(\"Recipe_Links_and_Names.csv\", index=False)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets run this function!. we save this to a csv to speed up work later as we are no longer depenadant on the websriver in another place and also in case of internet lose during the next test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Fresh Berry With Grand Marnier Sauce added\n",
      "Pumpkin Swirl Cheesecake added\n",
      "Graham Cracker Pie Crust added\n",
      "Marinated Tofu Jerky added\n",
      "Oven-Roasted Vegetables added\n",
      "Moroccan Black-Eyed Peas (Cowpeas) - Ful Gnaoua added\n",
      "Fried Fish Sandwich With Tartar Sauce and Hand-Cut Chips added\n",
      "Brandied Peaches added\n",
      "Totchos added\n",
      "Sweet and Tropical Pineapple Smoothie Recipe added\n",
      "Grilled Chicken Banh Mi Recipe added\n",
      "Chinese Stir-Fry Scallop With Vegetables Recipe added\n",
      "Pepper Steak Stir-Fry Recipe added\n",
      "Grilled Salmon Burgers With Radicchio Slaw and Sambal Mayonnaise added\n",
      "Kentucky Buck Cocktail Recipe added\n",
      "Sparkling Borage Cocktail added\n",
      "Chocolate Cherry Bread added\n",
      "Lunch Box-Worthy Falafel Kebabs added\n",
      "Chipotle Pumpkin Queso Dip added\n",
      "Flamin' Hot Cheetos Mac and Cheese Bites added\n",
      "Cherry Vinegar Recipe added\n",
      "Turkish Ramadan Flat Bread (Pide) added\n",
      "Carrot, Cabbage, and Kohlrabi Slaw With Miso Dressing Recipe added\n",
      "Esquites Recipe (Mexican Corn Off the Cob) added\n",
      "Gillie Fix Cocktail Recipe added\n",
      "Raspberry Snakebite Recipe added\n",
      "Stuffed Italian Sourdough Loaf Recipe added\n",
      "Strawberry Chicken Salad With Champagne Vinaigrette Recipe added\n",
      "Lasagne All'astice (Lobster Lasagna) Recipe added\n",
      "Pear and Pomegranate Champagne Shrub Recipe added\n",
      "Caramel Apple Jello Shots added\n",
      "Homemade Smoked Maple Bacon added\n",
      "Baked Sâ€™mores Skillet Dip Recipe added\n",
      "Garlic Chicken Primavera Pasta added\n",
      "Cold Soba Noodle Salad Recipe added\n",
      "Vegetarian Tofu Tacos added\n",
      "Bay Hill Hummer added\n",
      "A Recipe for Risotto Made With Amarone Wine (Risotto all'Amarone) added\n",
      "Cranberry Orange Bread added\n",
      "Tropical Raspberry Smoothie Recipe added\n",
      "Mexican Lasagna added\n",
      "Skillet Chicken With Creamed Spinach, Mushrooms, and Bacon Recipe added\n",
      "Grilled Prosciutto Wrapped Pork Chops added\n",
      "Cookie Dough Pops added\n",
      "Kentucky Coffee added\n",
      "Irish Ale Cocktail added\n",
      "Gluten-Free Fadge (Irish Potato Rolls) added\n",
      "Spring Greens: Beans With Lemon Ginger Butter Recipe added\n",
      "Super-Easy Balsamic Roasted Strawberry added\n",
      "Easy Classic Hollandaise Sauce With Tarragon added\n",
      "Easy Roasted Rhubarb Recipe added\n",
      "Traditional Smoked Mackerel Fishcakes added\n",
      "Pork and Onion Meatballs With Tomato Sauce added\n",
      "Easy and Traditional Blackberry Jelly added\n",
      "Yorkshire Ginger Parkin Biscuits added\n",
      "Easy Classic British Cheese Scones added\n",
      "Creamed Leeks and Smoked Haddock Recipe added\n",
      "Christmas Stollen Recipe added\n",
      "Anglesey Egg Recipe added\n",
      "White Bread (Made With Condensed Milk) Recipe added\n",
      "Garlic Chive Butter added\n",
      "Mint Butter Recipe added\n",
      "Easy Classic Elderflower Cordial added\n",
      "Welsh Leek and Stilton Soup added\n",
      "Super Apple and Almond Tart from Brendan Lynch added\n",
      "Venison Shepherd's Pie added\n",
      "Golden Beetroot Pasta added\n",
      "Gooseberry Compote added\n",
      "Bumbleberry Crumb Bars added\n",
      "Orange Sage Bread added\n",
      "World War I Era White Bread Recipe added\n",
      "Crispy Quinoa With Kale added\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_full_page_thespruceeats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have the receipie links we need to scrape each one we will break this into steps as much as possible first we need to soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup_object(url):\n",
    "    ###\n",
    "    #url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup\n",
    "    ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we will gather information by the order it is represented in our site using functions because we will loop over it soon!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our first function will gather the cook time prep time total time to cook and the amount of servings\n",
    "this was very messy because of the formating can change widly from one recepie to another time could look like 25 min or 2h 15 min or 2 hours 10 min or countless other variations that why we used a complex regex expression to combat this issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info1_fast(soup_obj):\n",
    "    lines=[]\n",
    "    results_items = soup_obj\n",
    "    results_items = soup_obj.find_all(class_='comp article__decision-block mntl-block')\n",
    "    if(results_items==[]):\n",
    "        soup = soup_obj\n",
    "        results_items = soup.find_all(class_='comp project-meta')        \n",
    "\n",
    "    for item in results_items:\n",
    "        item.find_all(class_='meta-text__data')\n",
    "        for sub_item in item:\n",
    "            if bool(sub_item.text.strip()):\n",
    "                clean_text = sub_item.text.strip().replace('\\n', '')\n",
    "                lines.append(clean_text)\n",
    "\n",
    "    if(len(lines)>1):\n",
    "        new_string = lines[0] + lines[1]\n",
    "        lines[0]= new_string\n",
    "\n",
    "    #use regex expressions to clean up the line we get it looks something like this\n",
    "    #['Prep: 15 minsCook: 20 minsTotal: 35 minsServings: 6 servingsYield: 1 cake', 'ratingsAdd a comment']\n",
    "    #prep = re.findall(r'Prep:\\s*(\\d+)\\s*mins', lines[0])[0]\n",
    "    cook_time_str = re.findall(r'Cook:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]\n",
    "    prep_time_str = re.findall(r'Prep:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]    \n",
    "    total_time_str = re.findall(r'Total:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]\n",
    "# Convert cook time to minutes\n",
    "\n",
    "\n",
    "    hours = int(cook_time_str[0]) if cook_time_str[0] else 0\n",
    "    minutes = int(cook_time_str[1]) if cook_time_str[1] else 0\n",
    "    cook_time_minutes = hours * 60 + minutes\n",
    "\n",
    "    hours = int(prep_time_str[0]) if prep_time_str[0] else 0\n",
    "    minutes = int(prep_time_str[1]) if prep_time_str[1] else 0\n",
    "    prep_time_minutes = hours * 60 + minutes\n",
    "\n",
    "\n",
    "    hours = int(total_time_str[0]) if total_time_str[0] else 0\n",
    "    minutes = int(total_time_str[1]) if total_time_str[1] else 0\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    #total = re.findall(r'Total:\\s*(\\d+)\\s*mins', lines[0])[0]\n",
    "    #servings = re.findall(r'Servings?:\\s*(\\d+?)\\s*(?:to\\s*\\d+)?\\s*(?:servings|ratings)', lines[0])[0] # sometimes instead of saying servings 6 they say servings 6 to 8 in this case we make it servings 6\n",
    "    #servings = re.findall(r'servings?:\\s*(\\d+?)\\s*(?:to\\s*\\d+)?\\s*(?:servings?|ratings)', lines[0], re.IGNORECASE)[0]\n",
    "    #text = \"The serving size is 3 servings per container.\"\n",
    "    \n",
    "    if(lines[0].count('serv')):\n",
    "        match = re.search(r'serv\\w*:\\D*(\\d+)', lines[0], re.IGNORECASE)\n",
    "        if match:\n",
    "            servings=(match.group(1))\n",
    "    else:\n",
    "        servings=1\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Prep': [prep_time_minutes],\n",
    "        'Cook': [cook_time_minutes],\n",
    "        'Total': [total_minutes],\n",
    "        'Servings': [servings]\n",
    "    })\n",
    "    df = df.astype(int)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for the second block of information which contains the rating for the dish \n",
    "this one was also messy because instead of writing a number they only display the star rating with 0.5 increments so we needed to count how many full stars and half stars there are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info2_fast(soup_obj):    \n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='comp js-feedback-trigger aggregate-star-rating mntl-block')    \n",
    "    #print(results_items.prettify())\n",
    "    for item in results_items:##result items size is 1\n",
    "        text=item.prettify()\n",
    "        full_stars=text.count('class=\"active\"')\n",
    "        half_stars=text.count('class=\"half\"')\n",
    "        return(full_stars+0.5*half_stars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up next is the rating count nothing to special about this one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_count(soup_obj):\n",
    "    soup = soup_obj\n",
    "    rating_elements = soup.find_all(\"div\", attrs={'class': \"comp aggregate-star-rating__count mntl-aggregate-rating mntl-text-block\"})\n",
    "    for rating_element in rating_elements:\n",
    "        rating_text = rating_element.text.strip()\n",
    "        try:\n",
    "            num_ratings = int(rating_text.split()[0])\n",
    "            return num_ratings\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up next we have the 3rd block of information which gives us the nutritional values there was an issue here that alcoholic beverages had no nutrinal value and the site had a lot of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info3_fast(soup_obj):\n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='nutrition-info__table--row')\n",
    "\n",
    "    nutritional_vals=[]    \n",
    "\n",
    "\n",
    "    for item in results_items:\n",
    "        nutritional_vals.append(item.text.strip())\n",
    "    new_list = []\n",
    "    for s in nutritional_vals:\n",
    "        # Split the string by the \\n character and add the two parts to a new list\n",
    "        parts = s.split('\\n')\n",
    "        # Add the new strings to the new list in the desired format\n",
    "        #[caleories:934,fat:134g,carbs:999]\n",
    "        new_list.extend([parts[1], parts[0]])\n",
    "    #[calories,934,far,134g,carbs,1123,]\n",
    "    df = pd.DataFrame({'nutrient': new_list[::2], 'value': new_list[1::2]})\n",
    "\n",
    "    # Set 'nutrient' column as index and transpose DataFrame\n",
    "    df = df.set_index('nutrient').T\n",
    "    if df.empty: #recepies like cocktails have no calories\n",
    "\n",
    "        df = pd.DataFrame(columns=['nutrient', 'Calories', 'Fat', 'Carbs', 'Protein'])\n",
    "\n",
    "        # add a row filled with zeros\n",
    "        df.loc[0] = ['value', 0, '0g', '0g', '0g']\n",
    "        df['Calories'] = df['Calories'].astype(np.int32)\n",
    "        return df \n",
    "\n",
    "    df['Calories'] = df['Calories'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is the 4th block of relevant information which holds the list of ingridients this was also not fun as the class names for some receipies changed and werent consistent resulting in a lot of hotfixed and hours of debugging which is why we added cond to check if its the old varient of the site or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info4_fast(soup_obj):\n",
    "    cond=0\n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='structured-ingredients__list text-passage')\n",
    "                                        #comp ingredient-list simple-list simple-list--bulleted  \n",
    "    #print(results_items)                                           \n",
    "    if(results_items==[]): #sometimes they like to change the class name\n",
    "        soup = soup_obj\n",
    "        results_items = soup.find_all(class_='simple-list__item js-checkbox-trigger ingredient text-passage')\n",
    "        cond=1\n",
    "    nutritional_vals=[]\n",
    "    if(results_items==[]):\n",
    "        return []\n",
    "    \n",
    "    final_lst=[]\n",
    "\n",
    "    for item in results_items:    \n",
    "        nutritional_vals.append(item.text.strip())\n",
    "        #print(item.text.strip())\n",
    "    if(cond==1):\n",
    "        return nutritional_vals\n",
    "    else:        \n",
    "        for i in nutritional_vals:\n",
    "            my_list = [s.strip() for s in i.split('\\n\\n\\n')]\n",
    "            final_lst.extend(my_list)\n",
    "            #print(final_lst)\n",
    "        \n",
    "        return(final_lst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also wanted to add some info of our own so we looked at the ingridients and tried to figure out if its dairy meat fur or dairy and meat as for keywords we just googled common meat or common dairy products and coppied the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recipe(ingredients):\n",
    "    dairy_keywords = [\"milk\", \"cheese\", \"yogurt\", \"cream\", \"butter\", \"whey\", \"casein\", \"curds\"]\n",
    "    meat_keywords = [\"beef\", \"chicken\", \"pork\", \"lamb\", \"turkey\", \"venison\", \"duck\", \"bacon\", \"sausage\",\n",
    "                     \"ham\", \"prosciutto\", \"pepperoni\", \"salami\", \"chorizo\", \"bresaola\", \"pastrami\",\n",
    "                     \"corned beef\", \"veal\", \"goose\", \"game\", \"elk\", \"bison\", \"rabbit\", \"boar\", \"guinea fowl\", \"quail\"]\n",
    "    \n",
    "    categories = {'Dairy': False, 'Meat': False, 'Fur': True}\n",
    "    \n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.lower()\n",
    "        if any(keyword in ingredient for keyword in dairy_keywords):\n",
    "            categories['Dairy'] = True\n",
    "            categories['Fur'] = False\n",
    "        elif any(keyword in ingredient for keyword in meat_keywords):\n",
    "            categories['Meat'] = True\n",
    "            categories['Fur'] = False\n",
    "            \n",
    "    df = pd.DataFrame(categories, index=[0])\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to merge all of this data into a single row of a dataframe and again we will be looping over this one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fast(url,recepie_name):\n",
    "    df = pd.DataFrame(columns=['Name','Prep', 'Cook', 'Total', 'Servings', 'Rating','Rating_Count','Dairy','Meat','Fur', 'Calories', 'Fat', 'Carbs', 'Protein', 'Ingredients'])\n",
    "    #Dairy\n",
    "    #Meat\n",
    "    #Fur\n",
    "    soup_obj= load_soup_object(url)\n",
    "\n",
    "    recipe_df = get_info1_fast(soup_obj)\n",
    "\n",
    "    ratings_list=get_info2_fast(soup_obj)\n",
    "\n",
    "    nutrition_df = get_info3_fast(soup_obj)\n",
    "\n",
    "    ingredients = get_info4_fast(soup_obj)\n",
    "\n",
    "    rating_num = get_rating_count(soup_obj)\n",
    "\n",
    "    meatdaity_df =analyze_recipe(ingredients)\n",
    "\n",
    "    if(ingredients==[]):\n",
    "        ingredients=['','','','']\n",
    "        print(type(nutrition_df['Calories'][0]))\n",
    "    new_row = {\n",
    "        'Name':recepie_name,\n",
    "        'Prep': recipe_df['Prep'][0],\n",
    "        'Cook': recipe_df['Cook'][0],\n",
    "        'Total': recipe_df['Total'][0],\n",
    "        'Servings': recipe_df['Servings'][0],\n",
    "        'Rating': ratings_list,\n",
    "        'Rating_Count':rating_num,\n",
    "        'Dairy':meatdaity_df['Dairy'][0],\n",
    "        'Meat':meatdaity_df['Meat'][0],\n",
    "        'Fur':meatdaity_df['Fur'][0],\n",
    "        'Calories': nutrition_df['Calories'][0],\n",
    "        'Fat': nutrition_df['Fat'][0],\n",
    "        'Carbs': nutrition_df['Carbs'][0],        \n",
    "        'Protein': nutrition_df['Protein'][0],\n",
    "        'Ingredients': [ingredients]\n",
    "    }\n",
    "    #print(new_row)\n",
    "\n",
    "    # add the new row to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "    #df_concat = pd.concat([df1, df2], keys=['df2'])\n",
    "    df['Prep'] = df['Prep'].astype(float)\n",
    "    df['Cook'] = df['Cook'].astype(float)\n",
    "    df['Total'] = df['Total'].astype(float)\n",
    "    df['Servings'] = df['Servings'].astype(float)\n",
    "    df['Rating'] = df['Rating'    ].astype(float)\n",
    "    df['Calories'] = df['Calories'].astype(float)\n",
    "\n",
    "    # display the updated DataFrame\n",
    "    #print(\"another line added\")\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we want to do this for each receipe that was found up until now here comes another function :)\n",
    "it will take a while so again to speed up work later we are saving it to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scrape(csv_file_name):\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "    count=0\n",
    "\n",
    "    with open(csv_file_name, encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            recipe = ','.join(row).strip().replace('\\x9c', '')  # Join recipe name and link with a comma, remove problematic characters\n",
    "            last_comma = recipe.rfind(',')  # Find the index of the last comma in the recipe string\n",
    "            if last_comma != -1:\n",
    "                recipe_name = recipe[:last_comma].strip()  # Get the recipe name before the last comma\n",
    "                recipe_link = recipe[last_comma+1:].strip()  # Get the recipe link after the last comma\n",
    "                recipe_names.append(recipe_name)\n",
    "                recipe_links.append(recipe_link)\n",
    "            else:\n",
    "                print(f\"Invalid row: {row}\")\n",
    "\n",
    "    final_df=pd.DataFrame()\n",
    "    for name, link in zip(recipe_names, recipe_links):\n",
    "        # print(f\"Recipe name: {name}\")\n",
    "        # print(f\"Recipe link: {link}\")\n",
    "        temp_df=merge_fast(link,name)\n",
    "        temp_df.set_index('Name', inplace=True)  # set the index of temp_df to the recipe name\n",
    "        if(final_df.empty):\n",
    "            final_df=temp_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df,temp_df])\n",
    "        print(final_df)\n",
    "\n",
    "    #final_df.to_pickle('dataframe.pkl')\n",
    "    #final_df.to_pickle('dataframe.pkl', protocol=4, encoding='utf-8')\n",
    "    final_df.to_csv('my_data.csv', index=True, encoding='utf-8')\n",
    "    print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'Recipe_link': No scheme supplied. Perhaps you meant http://Recipe_link?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fast_scrape(\u001b[39m'\u001b[39;49m\u001b[39mRecipe_Links_and_Names.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m, in \u001b[0;36mfast_scrape\u001b[1;34m(csv_file_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m final_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m name, link \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(recipe_names, recipe_links):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# print(f\"Recipe name: {name}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# print(f\"Recipe link: {link}\")\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     temp_df\u001b[39m=\u001b[39mmerge_fast(link,name)\n\u001b[0;32m     24\u001b[0m     temp_df\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# set the index of temp_df to the recipe name\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m(final_df\u001b[39m.\u001b[39mempty):\n",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m, in \u001b[0;36mmerge_fast\u001b[1;34m(url, recepie_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mPrep\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mServings\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRating_Count\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDairy\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMeat\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mFur\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCalories\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCarbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProtein\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIngredients\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39m#Dairy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#Meat\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#Fur\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m soup_obj\u001b[39m=\u001b[39m load_soup_object(url)\n\u001b[0;32m      8\u001b[0m recipe_df \u001b[39m=\u001b[39m get_info1_fast(soup_obj)\n\u001b[0;32m     10\u001b[0m ratings_list\u001b[39m=\u001b[39mget_info2_fast(soup_obj)\n",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m, in \u001b[0;36mload_soup_object\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_soup_object\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m     \u001b[39m###\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39m#url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m      5\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m*\u001b[39me\u001b[39m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No scheme supplied. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerhaps you meant http://\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'Recipe_link': No scheme supplied. Perhaps you meant http://Recipe_link?"
     ]
    }
   ],
   "source": [
    "fast_scrape('Recipe_Links_and_Names.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have our dataframe lets see what we are working with we will try to visualize the data and clean it up later ofc we want some functions we will make functions for scatter plot \n",
    "histogram and a 1 column pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter_2_params(col_name_1,col_name_2):\n",
    "    df=pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df.plot.scatter(x=col_name_1, y=col_name_2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histo_1_params(col_name):\n",
    "    # read in your dataframe from a csv file\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # choose the column you want to use for the histogram\n",
    "\n",
    "    # sort the column values into bins\n",
    "    bin_values, bin_edges = np.histogram(df[col_name], bins='auto')\n",
    "\n",
    "    # create the histogram using the sorted bins\n",
    "    plt.hist(df[col_name], bins=bin_edges)\n",
    "\n",
    "    # add labels and title to the histogram\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + col_name)\n",
    "\n",
    "    # display the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_1_params(col_name):\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # choose the column you want to use for the pie chart    \n",
    "\n",
    "    # get the count of unique values in the column\n",
    "    value_counts = df[col_name].value_counts()\n",
    "\n",
    "    # create the pie chart\n",
    "    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "    # add title to the pie chart\n",
    "    plt.title('Pie Chart of ' + col_name)\n",
    "\n",
    "    # display the pie chart\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in addition we wanted a pie chart of how much is meat dairy meat and dairy and fur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_meat_dairy_fur():\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    \n",
    "    # Calculate the number of recipes in each category\n",
    "    meat_count = len(df[df['Meat'] == True])\n",
    "    dairy_count = len(df[df['Dairy'] == True])\n",
    "    fur_count = len(df[df['Fur'] == True])\n",
    "    dairy_meat_count = len(df[(df['Dairy'] == True) & (df['Meat'] == True)])\n",
    "\n",
    "    # Create a list of category counts and labels\n",
    "    counts = [meat_count, dairy_count, fur_count, dairy_meat_count]\n",
    "    labels = ['Meat', 'Dairy', 'Fur', 'Dairy&Meat']\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "    # Add a title to the chart\n",
    "    plt.title('Recipe Categories')\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets run all of these and see if we can already draw some conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
